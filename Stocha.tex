\documentclass{article}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[utf8]{inputenc}
\begin{document}
\section{Kombinatorik}
\begin{tabular}{c| c |c}
  & ohne & Zurück legen \\ \hline
  Reigenfolge & $\frac{n!}{(n-k)!}$&$n^k$ \\
  ohne&${n \choose k}$ & ${n + k -1 \choose k}$ \\
  
\end{tabular}

\section{Diskrete Zufallsvariablen}
\subsection{Axiomatscher Ansatz}
\begin{itemize}
\item $P(E)\geq 0$
\item $P(\Omega) = 1$
\item $P(E\cup F) = P(E)+P(F), \; wenn\;  E\cap F = \emptyset $
\item $\Omega = E \cup \overline{E}$
\end{itemize}


\subsection{Gemeinsame und bedingte Wahrscheinlichkeiten}
\begin{itemize}
\item$P(EF) = P(E\cap F) = P(F \cap E) = P(FE)$
\item $P(E|F) = \frac{P(EF)}{P(F)} = \frac{P(FE)}{P(F)}$
\item $P(EF) = P(E|F)P(F)$
\end{itemize}




\subsection{statistische Unabhängigkeit}
\begin{itemize}
\item $P(EF) = P(E) P(F)$
\end{itemize}

\subsection{Bayes-Regel}
\begin{itemize}
\item $P(A_i|B_j) = \frac{P(B_j|A_i)P(A_i)}{P(B_j)}$
\item $P(F|E) = \frac{P(E|F) P(F)}{P(E|F) P(F)+P(E|\overline{F})P(\overline{F})}$
\end{itemize}


\subsection{Gesetz der Gesamtwsk}
\begin{itemize}
\item $P(F) = \sum_{i=1}^K P(F|E_i)P(E_i)$
\end{itemize}



\subsection{Diskrete Zufallsbvariablen}

\begin{itemize}
\item Eine Zufallsvaribale ZV $X(\omega)$ ist eine Funktion, die jedem möglichen \newline Ergebnis $\omega \in \Omega$ eines Zufallsexperimentes eine reelle Zahl $X(\omega)$ zuordnet.
\item Jede reelle Zahl $X(\omega)$ hate eine zugehörige Wahrscheinlichkeit.\newline Wir schreiben diese als $P(X(\omega) = x) = P_X(x)$
\item $P(4\leq X \leq 6) = P(X\leq 6) -P(X \leq 3)$
\end{itemize}

\subsection{Wahrscheinlichkeitsverteilung}
\begin{itemize}
\item $P_X(x) \hat{=} P(X=x)$
\item $ 0 \leq P_X(x) \leq 1$
\item $\sum_X P_X(x) = 1$
\end{itemize}

\subsection{Poisson-ZV}
\begin{itemize}
\item $P_X(x) = \frac{\lambda^x e^{-\lambda}}{x!}$
\item $\sum_{-\infty}^\infty x\frac{\lambda^x e^{-\lambda}}{x!} = 1$
\end{itemize}


\subsection{Zwei diskrete ZV}
\begin{tabular}{c | c}
WskVerteilung & Formel \\ \hline
Gemeinsame & $P_{XY}(x,y) \hat{=} P(X=x,Y=y)$\\
Bedingte& $P_{X|Y}(x|y) = \frac{P_{XY}(x,y)}{P_Y(y)}$\\
Rand & $P_X(x,y) = \sum_Y P_{XY}(x,y)$\\

\end{tabular}

\subsection{binomial Verteilung}
\begin{itemize}
\item $P_X(k) = {n \choose k} p^k (1-p)^{n-k} = \frac{n!}{k! (n-k)!} p^k (1-p)^{n-k} $
\end{itemize}

\subsection{Erwartungswerte und Momente}
\begin{tabular}{c|c}
& Formel \\ \hline
Mittelwert & $\mu_X = E\{X\} \hat{=} \sum_X xP_X(x)$ \\
Varianz & $var(X) \hat{=} E\{(X-\mu_X)^2\} = \sum_X (x-\mu_X)^2P_X(x)$\\
Standard Abw. & $\sigma_X = \sqrt{var(X)}$\\
Korrelation & $\mu_{XY} = E\{XY\} = \sum_X \sum_Y xyP_{XY}(x,y)$\\
Kovarianz & $\sigma_{XY} = E\{(X-\mu_X)(Y-\mu_Y)\} =\sum_X \sum_Y (x-\mu_X)(y-\mu_Y)P_{XY}(x,y) $\\
Linearität & $E\{aX+bY\} = aE\{X\} + bE\{Y\}$ \\
\end{tabular}


\begin{itemize}
\item $\sigma_X^2=\mu_{X^2} - \mu_X^2$
\item $E[x^2] = E[x(x-1)]+E[x]$
\item $\sigma_ {XY} = E[(X-\mu_X)(Y-\mu_Y)] = \mu_{XY} - \mu_X\mu_Y$
\end {itemize}


\subsection{Unkorreliertheit}
\begin{itemize}
\item $E\{XY\} = E\{X\}E\{Y\}$, d.h $ \sigma_{XY} = 0$, dann sind X und Y unkorreliert
\item statistische Unabhängigkeit bedeutet Unkorreliertheit aber \textbf{nicht umgekehrt}
\item Korrelationskoeffizient $\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$
\item $-1\leq  \rho_{XY} \leq  1$
\end{itemize}

\subsection{Lineare Schätzung}
\begin{itemize}
\item opt Schätzer, der MSE minimiert: \newline $\hat{Y} = \mu_Y + \rho_{XY}\frac{\sigma_Y}{\sigma_X} (X-\mu_x) $
\item dessen MSE = $\sigma_Y^2(1-\rho_{XY}^2)$
\end{itemize}

\subsection{}
\begin{itemize}
\item Stichprobenmw: $\hat{\mu}_n = \frac{1}{n} \sum_{i=1}^n x_i$
\item Stichprobenvar: $\hat{\sigma}_n^2 = \frac{1}{n-1} \sum_{i=1}^n(x_i-\hat{\mu}_n)^2$
\end{itemize}


\section{Zeitdiskrete Markoff-Ketten}
\subsection{Definition}

Eine Folge von ganzwertigen ZVn. \newline
Ereignis = Zustand \newline
Zeithomogen, wenn keine Abhängikeit von der Zeit besteht \newline
$P(X_{n+1} = j | X_n = i)$ sind Übergangswahrscheinlichkeiten \newline
$P(X_n = i)$ sind Zustandwahrscheinlichkeiten \newline
$P(X_{n+1} = | X_n = i_n , ..., X_0 = i_0) = P(X_{n+1} = i_{i+1} | X_n = i_n)$


\subsection{station{\"a}re Verteilung}
\begin{itemize}
\item $q_n = q_0 P^n$
\item Station{\"a}re Verteilung, wenn $\pi = \pi P$ 
\item $\pi = \pi P \Leftrightarrow \pi(P - I) = 0$
\item irreduzibel = Jeder Zusatand erreichbar
\item Eine irreduzible, aperiodische Kette mit einer endlichen Anzahl von Zuständen hat eine eindeutige stationäre Verteilung.
\item eindeutige stationäre Verteilung, wenn irreduzibel und aperiodisch
\end{itemize}

\subsection{Berechnung der stationären Verteilung}


\section{Stetige Zufallsvariablen}
\subsection{Wkdichte- und kumulative Verteilungsfunktionen}
\begin{tabular}{c | c}
Funktion & Formel \\ \hline
kummulative Verteilungs& $F_X (x) \hat{=}  P(X \leq x ) = \int_{-\infty}^x f_X(\xi)d\xi $ \\
Wskdichte  &$f_X(x) = \frac{d}{dx} F_X(x)$\\
\end{tabular}

\subsubsection{Eigenschaften}
\begin{itemize}
\item $P(a < X \leq b) = P(X\le b) - P(X \le a) = F_X (b) - F_X(a)$
\item $P(X > a) = 1-P(X \le a) = 1- F_X(a)$
\item $F_X(x) $ ist nicht abnehmend
\item$ f_x(x) \ge 0 $ aber nicht notwendigerweise $f_X(x) \leq 1$
 \item $F_X (-\infty) = 0$ und $ F_X(\infty) = \int_{-\infty}^\infty f_X(x) dx =1$
\end{itemize}

\subsection{Zwei Zufallsvariablen}
\begin{tabular}{r l}

gem. k. Vf.& $F_{XY} (x,y) = P(X \leq x, Y\leq y)$\\
gem. Wdf. & $f_{xy}(x,y) = \frac{\partial}{\partial_x} \frac{\partial}{\partial_y} F_{XY}(x,y)$\\
Rand-Wdf. & $F_X(x) = \int_{-\infty}^\infty f_{XY}(x,y) dy$\\
bedingte Wdf. & $f_{X|Y}(x|y) = \frac{F_{XY}(x,y)}{f_Y(y)}$\\

\end{tabular}
\subsection{statistische Unabh{\"a}ngikgeit}
\begin{itemize}
\item $F_{XY}(x,y) = F_X(x)F_Y(y)$ 
\item $f_{XY}(x,y) = f_X(x)f_Y(y)$ 
\item $f_{X|Y}(x|y) = f_X(x) $ 
\end{itemize}

\subsection{Erwartungswerte}
\subsubsection{Eine stetige ZV}

\begin{tabular}{c | c}
Erwartungswert Operator & $E\{g(X)\} \hat{=} \int_{-\infty}^\infty g(x)f_X(x) dc$ \\
Moment 1. Ordnung & $ \mu_X = E\{X\} = \int_{-\infty}^\infty xf_X(x) dx$ \\
Moment 2. Ordnung& $ \mu_{x^2} = E\{X^2\} = \int_{-\infty}^\infty x^2f_X(x) dx$ \\
Varianz& $ var(x) = \sigma_x^2 = E\{(X-{\mu_X})^2\}$ \\
\end{tabular}
\subsubsection{Zwei stetige ZVn}

\begin{tabular}{c | c}
& $E\{g(X,Y)\} = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y)f_{XY}(x,y) dx dy$ \\
Korrelation& $\mu_{XY} = E\{XY\} = \int_{-\infty}^\infty \int_{-\infty}^\infty xyf_{XY}(x,y) dx dy$ \\
Kovarianz & $\sigma_{XY} = E\{(X-\mu_x)(Y-\mu_y)\} =\int_{-\infty}^\infty \int_{-\infty}^\infty (X-\mu_x)(Y-\mu_y)f_{XY}(x,y) dx dy $\\
\end{tabular}

\subsection{Tscherbyscheff Ungleichung}
Sie X eine ZV mit dem Mittelwert $\mu_X$ und einer endlichen Varianz $\sigma_X^2$. Für ein beliebiges $\delta >0$
$$P[|X-\mu_x|\geq \delta] \leq \frac{\sigma_X^2}{\delta^2}$$

\subsection{Wichtige Verteilungen}
\subsubsection{Gleichverteilung}
 
$
    f_X(x) = \begin{cases}
               \frac{1}{b-a}   & a \le x \le b\\
               0 & \text{sonst}
           \end{cases}
$
\subsubsection {Exponentialverteilung}
$
    f_X(x) = \begin{cases}
               \lambda e^{-\lambda x}   &  x \geq 0\\
               0 & \text{sonst}
           \end{cases}
$ $\quad$
$           
            F_X(x) = \begin{cases}
               1- e^{-\lambda x}   &  x \geq 0\\
               0 & \text{sonst}
           \end{cases}
$

\subsubsection{Gau\ss-Verteilung (Normalverteilung)}
$$X \sim N(\mu_X,\sigma_X^2)$$
$$f_X(x) = \frac{1}{\sqrt{2\pi} \sigma_X}exp({-\frac{1}{2}(\frac{x-\mu_x}{\sigma_X})^2})$$
\begin {tabular}{c c}
 & $\phi(z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac{s^2}{2}} ds$  \\
 Fehlerfunktion & $erf(z) = \frac{2}{\sqrt{\pi}}\int_0^z e^{-s^2}ds$\\
 komplementäre Fehlerfunktion & $erfc(z) = 1-erf(z) = \frac{2}{\sqrt{\pi}}\int_z^\infty e^{-s^2}ds  = 2\phi(-\sqrt{2}z)$\\
 Q-Funktion & $Q(z) = 1-\phi(z) = \frac{1}{2}erfc(\frac{z}{\sqrt{2}})$ \\
 \end{tabular}
\subsubsection{Eigenschaften}
\begin{itemize}
\item vollständig bestimmt durch Mittelwert und Varianz
\item Wenn X und Y gem. Gau\ss-verteilt sind und unkorreliert sind, dann sind sie unabhängig
\item Wenn X und Y gem. Gau\ss-verteilt sind, dann sind auch die Randverteilungen Gau\ss isch
\item Wenn X und Y gem. Gau\ss-verteilt sind, dann ist es $X|Y$ auch
\item Wenn X Gau\ss-verteilt ist, dann ist aX+b auch (mit $ Mittelwert \: a\mu_X + b \: und \: Varianz \: a2^2\sigma_X^2$)

\end{itemize}
\subsection{Momenterzeugende Funktion}
$$\phi(s) = E\{e^{sX}\} = \int_{\infty}^{\infty}e^{sx} f_X(x) dx,\quad s \in C $$
\subsubsection{n-te Moment}
$$E\{X^n\} = \frac{d^n}{ds^n} \phi(s)|_{s=0}$$
\subsubsection{Charakteristische Funktion}
$$\psi(\omega) = E\{e^{j\omega X}\} = \int_{-\infty}^{\infty} e^{j\omega x}f_X(x) dx =  \psi(j\omega), \quad \omega \in R$$
\begin {itemize}
\item  Die c.F existiert immer, weil $|\psi(\omega)| = |E\{ e^{j\omega X}\}| \le E\{e^{j\omega X}|\} = 1$
\item Aufgrund der Symmetrie der FT $\psi(\omega) = \psi^*(-\omega)$
\item c.F. ist reel (und gerade) genau dann wenn die Wdf. gerade ist: $f_X(x) = f_X(-x)$
\item weil $\psi(\omega) = \psi(j\omega)$, können wir die Momente auch mit der c.F erzeugen: $E\{X^n\} = \frac{d^n}{d\omega^n}(-j)^n\psi(\omega)|_{\omega = 0}$
\end{itemize}
\subsection{Gesetz der gro\ss en Zahlen und zentraler Grenzwertsatz}
\subsubsection{schwaches Gesetz der gro\ss en Zahlen}
$\hat{\mu}_N = \frac{1}{N} \sum_{n = 1}^N X_n$
\subsection{Zentraler Grenzwertsatz}
$Z = \frac{1}{\sqrt{N}}\sum_{n = 1}^N(X_n -\mu)$

\section {Statistik}
\subsection{Parameterschätzung}
\begin{tabular}{c c }
Stichprobe& Reihe von Messungen \\
Statistik & beliebige Funktion der Daten \\
Stichprobenmittel & $\hat{M}_n = \frac{1}{n} \sum_{i=1}^n X_i$\\
Stichprobenvarianz & $\hat{\sum}_n^2 = \frac{1}{n-1} \sum_{i = 1}^n(X_i-\hat{M}_n)^2$ \\
		& $\hat{\sum}_n^2 = \frac{1}{n} \sum_{i = 1}^n(X_i-\hat{\mu}_X)^2$
\end{tabular}
\subsection{Erwartungstreue und Konsistenz}
Wenn das Mittel des Schätzers dem Mittelwert des Parameters, den er schätz, entspricht, dann nennen wir den Schätzer \textbf{ erwartungstreu}.



\subsection{Erwartungstreue und Konsistenz}
\begin{itemize}
\item $ E[\hat{M}_n] = \frac{1}{n} \sum_{i=1}^n \mu_X = \mu_X$
\item $E[\hat{\sum}_n^2] = \sigma_X^2$
\item Stichproben mittel und -varianz sind konstistente Schätzer von Ensemble-Mittel und Varianz
\item Erwartungstreue $\neq$ Konsistenz
\end{itemize}



\subsection{Konfidenzintervalle}
\begin{itemize}
\item Wir schreiben $\mu_X = \hat{M}_n \pm $ mit $100(1-\alpha)\% Wsk$
\item wenn wir meinen $P(\mu_X \in [\hat{M}_n - \delta,\hat{M}_n + \delta]) = 1 -\alpha $
\item Konfidenzintervall: $ [\hat{M}_n - \delta,\hat{M}_n + \delta]$
\item Konfidenzniveau: $1-\alpha$
\end{itemize}


\subsection{Hypothesentest für das Mittel}
\begin{itemize}
\item $\mu \leq \mu_0$ oder  $ \mu > \mu_0$ 
\item $\mu =\mu_0$ oder $\mu \neq \mu_0$ 
\item Fehler 1. Art (falscher Alarm) Entscheidung für $H_1$, obwohl $H_0$ wahr ist
\item Fehler 2. Art (versäumte Detektion) Entscheidung für $H_0$, obwohl $H_1$ wahr ist
\item einseitiger Test: $y_\alpha$
\item zweiseitiger Test: $y_\frac{\alpha}{2}$
\end{itemize}

\subsection{Histogramm}
\begin{itemize}
\item Schätzung einer WskVerteilunf -dichte wie oft BEobachtungen in eine Klasse fallen
\end{itemize}

\section{Zufallsvektoren}
\subsection{Bivariante Gau\ss verteilung}
\begin{itemize}
\item gemeinsam gau\ss isch $f_{XY}(x,y) = \\
 \frac{1}{2\pi\sigma_X\sigma_Y \sqrt{1-\rho_{XY}^2}} exp\{- \frac{1}{2(1-\rho_{XY}^2)} [(\frac{x-\mu_x}{\sigma_X})^2 - 2\rho_{XY} \frac{(x-\mu_x)(y-\mu_y)}{\sigma_X\sigma_Y}+(\frac{y-\mu_y}{\sigma_Y})^2] \}$
\item Randverteilung $f_X(x) = \int_{-\infty}^{\infty} f_{XY}(x,y) dy = \frac{1}{\sqrt{2\pi}\sigma_X} exp\{ - \frac{1}{2} (\frac{x-\mu_X}{\sigma_X})^2\}$
\item Wenn X und Y unkorreliert sind, d.h. $\rho_{XY} = 0$ ,dann ist $f_{XY}(x,y) = f_X(x)f_Y(y)$, d.h. X und Y sind unabgängig
\end{itemize}

\subsection{Bedingte Gau\ss - Wdf.}
\begin{itemize}
\item$F_{Y|X}(y|x) = \frac{f_{XY}}{f_X(x)}$
\item Gau\ss isch Mittel  $E\{Y|X=x\} = \mu_Y + \rho_{XY} \frac{\sigma_Y}{\sigma_X}(x-\mu_x)$
\item Varianz $\sigma_Y^2(1-\rho_{XY}^2)$
\item Für $\hat{Y} = E\{Y|X\} ist \hat{Y} = aX+b$, der lin Schätzer, $E\{(\hat{Y}-Y)^2\}$ MSE
\end{itemize}

\subsection{Gemeinsame Verteilungen und Dichten}
\begin{itemize}
\item Sei $\textbf{X} = [X_1, ..., X_n]^T, \textbf{Y} = [Y_1, ..., Y_n]^T$
\item  k. Vf $F_\textbf{X}(\textbf{x}) = P(X_1 \leq x_1, ..., X_n \leq x_n) $
\item  Wdf $f_\textbf{X}(\textbf{x}) = \frac{\partial}{\partial_{x_1}} ... \frac{\partial}{\partial_{x_n}} F_\textbf{X}(\textbf{x})$
\item gem  k. Vf $F_\textbf{XY}(\textbf{x,y}) = P(X_1 \leq x_1, ..., X_n \leq x_n, Y_1 \leq y_1, ..., Y_n \leq y_n) $
\item gem Wdf $f_\textbf{XY}(\textbf{x,y}) = \frac{\partial}{\partial_{x_1}} ... \frac{\partial}{\partial_{x_n}} \frac{\partial}{\partial_{y_1}} ... \frac{\partial}{\partial_{y_n}}F_\textbf{XY}(\textbf{x,y})$
\end{itemize}

\subsection{Momente}
\begin{itemize}
\item $\mu_{X_i} = E\{X_i\}, i = 1, ...,n$
\item $\sigma_{X_i}^2 = E\{(X_i - \mu_{X_i})^2 \}$
\item $\sigma_{X_i X_j} = E\{ (X_i-\mu_{x_i})(X_j-\mu_{X_j})\}$
\end{itemize}

\subsection{Mittelwertvektor und Kovarianz}
\begin{itemize}
\item  \bm{$\mu_X$} = $E \{ \bm{X} \} $
\item $\bm{R_{XX}} = E\{(\bm{X}- \bm{\mu_X})(\bm{X}- \bm{\mu_X})^T  \} = E\{\bm{XX}^T \}-\bm{\mu_X\mu_X}^T$
\item Element (i,j) von $\bm{R_{XX}}$ ist die Kovarianz zwischen $X_i X_j$
\item $\bm{R_{XX}}$ ist symmetrisch: $\bm{R_{XX}} = \bm{R_{XX}}^T$
\end{itemize}


\subsection{Kovarianzmatrix}
\begin{itemize}
\item Korrelationsmatrix: $ E\{\bm{XX}^T\}$
\item Kreuz-Kovar: $\bm{R_{XY}} = E\{ \bm{(X-\mu_X)(Y-\mu_Y)}^T \}$
\item Unkorreliert: $E\{ \bm{XY}^T\} = E\{ \bm{X}\}E\{\bm{Y}^T\}$ bedeutet aber nicht unabhängig
\item orthogional $E\{\bm{XY}^T\} = 0$
\item Kovar, Korrelationsmatrizen sind symmetrisch und pos semidefinit: \\ $\bm{z}^T \bm{R_{XX}z}\geq 0 \;\forall \bm{z \neq 0}$
\end{itemize}


\subsection{Transformation von Zufallsvektoren}
\begin{itemize}
\item Sei $\bm{y = g(x)}$, g invertierbar mit inverser Funktion h \\ $f_ {\bm{Y}}(\bm{y}) = f_{\bm{X}}(\bm{h(y)})|det(J_{\bm{h}}(\bm{y}) |$
\item
\end{itemize}

\subsection{Multivariante Gau\ss Verteilung}
\begin{itemize}
\item n-dim Wdf $f_{\bm{X}}(\bm{x}) = \frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{det(\bm{R_{XX}})}}exp\{-\frac{1}{2}(\bm{x-\mu_x})^T \bm{R_{XX}^{-1}(x-\mu_X)} \}$
\end{itemize}

\subsection{Verhalten bei linearer Transformation}
\begin{itemize}
\item Sei $\bm{A}$ eine m x n Matrix vom Rang m. Dann Zufallsvektor $\bm{Y} = A \bm{X}$ eine m-dim Gau\ss Verteilung mit MWvek $\bm{\mu_Y} = A \bm{\mu_X}$ und Kovarmat $\bm[{R_{YY} = A R _{XX} A}^T$
\end{itemize}

\subsection{bedingte Wdf}
\begin{itemize}
\item $f_{X|Y}(x|y)$ ist gau\ss isch mit Mittelwertvek $\bm{W_y}$ und Kovarmat $\bm{Q}$
\item $f_{X|Y}(x|y) = \frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{det(\bm{Q)}}}exp\{(\bm{x-W_y})^T \bm{Q}^{-1}(\bm{x}-\bm{W}_y) \} $
\end{itemize}

\section{Zufallsprozess}
\begin{itemize}
\item Zufallsprozess (ZP, stochastisches Signal) ist eine Schar/Ensemble von Musterfunktionen
\item Jede Musterfunktion(Beobachtung) $x_n(t)$ ist deterministisch
\item Für einen Zeitpunkt $t=t_0$ wird aus dem ZP $X(t)$ eine Zufallsvariable $Z = X(t_0)$
\item gibt es zeitkontinuierlich udn diskret
\end{itemize}

\subsection{Statistik}

\begin{tabular}{c |c }
Funktion & Formel \\ \hline
kum. Vert. & $F_{X(t)}(x) \hat{=}P(X(t) \leq x )$\\
wdf & $f_{X(t)}(x) \hat{=} \frac{d}{dx}F_{X(t)}(x) $\\
gem kum. & $F_{X(t_1)X(t_2)}(x_1,x_2) \hat{=}P(X(t_1) \leq x_1, X(t_2) \leq x_2)  $\\
gem. Wdf & $f_{X(t_1)X(t_2)}(x_1,x_2) \hat{=} \frac{\partial^2}{\partial x_1 \; \partial x_2}F_{X(t_1)X(t_2)}(x_1,x_2) $

\end{tabular}

\subsection{Momentfunktionen}
\begin{tabular}{c |c}
Funktion & Formel \\ \hline
Mittelwert & $\mu_X(t) \hat{=} E[X(t)] = \int_{-\infty}^\infty x f_{X(t)}(x) dx$\\
AutoKorrelation &$ m_{xx}(t_1,t_2) \hat{=} E[X(t_1)X(t_2)] = \int\int_{-\infty}^\infty x_1x_2f_{X(t_1)X(t_2)}(x_1,x_2)dx_1 dx_2$\\
AutoKovar& $C_c(t) \hat{=} X(t)-\mu_X(t):$ \\
& $r_{xx}(t_1,t_2) \hat{=} E[X_c(t_1)X_c(t_2)] = E[(X(t_1)-\mu_X(t_1))(X(t_2)-\mu_X(t_2))]$ \\
& $r_{XX}(t_1,t_2) = m_{xx}(t_1,t_2) - \mu_X(t_1)\mu_X(t_2)$ \\
\end{tabular}

\subsection{WSS Prozesse}
\begin{itemize}
\item Ein ZP ist WSS, wenn sein Mittel zeitinvariant ist und seine AKF verschiebungsinvariant ist
\item Sei $t_1 = t + \tau$ udn $ t_2 = t$ Dann ist AKF verschiebungsinvariant, wenn sie nur von der Zeitverschiebung $\tau$ abghängt \\
$m_{XX} \hat{=} E[X(t+\tau)X(t)] = \int\int_{-\infty}^\infty x_1x_2f_{X(t+\tau)X(t)}(x_1,x_2)dx_1 dx_2 $
\item $r_{XX}(\tau) = m_{XX}(\tau)-\mu_X^2$
\end{itemize}


\subsection{Gem. Momentfunktion}
\begin{itemize}
\item$m_{XY} \hat{=} E[X(t+\tau)Y(t)] = \int\int_{-\infty}^\infty xyf_{X(t+\tau)Y(t)}(x,y)dx_1 dy $
\item $r_{XY}(\tau) \hat{=} E[X_C(t+\tau)Y_C(t)] = E[(X(t+\tau)-\mu_X)(Y(t)-\mu_Y)]$
\item $r:{XY}(\tau) = m_{XY}(\tau) -\mu_X\mu_Y$
\item X(t) und Y(t) können einzeln WSS sein, ohne gemeinsam WSS zu sein
\end{itemize}


\subsection{Eigenschaften der Korrelationsfunktion}
\begin{itemize}
\item $|m_{XY}(\tau)| \leq \sqrt{m_{XX}(0)m_{YY}(0)}$
\item$|m_{XX}(\tau)| \leq m_{XX}(0) = P:X =$ Leistung von $X(t)$, welche für WSS konstant
\item $m_{XX}(\tau) = m_{XX}(-\tau)$ (reell und gerade)
\item $ \forall \tau \; m_{XY}(\tau) = 0 \Rightarrow$ orthogonal
\item $ \forall \tau \; m_{XY}(\tau) = \mu_X  \mu_Y \Rightarrow $  unkorreliert
\end{itemize}


\subsection{Ergodizität}
\begin{tabular}{ c |c}
Ergodizität & Formel \\ \hline
Mittel & $\hat{\mu}_X \hat{=} \frac{1}{2T} \int _{-T}^T x(t)dt \; {T \to \infty} \; \mu_X(\tau) = E[X(t)] $\\
Korrelation & $\hat{m}_{XX}(\tau) \hat{=} \frac{1}{2T} \int _{-T}^T x(t+\tau)x(t)dt \; {T \to \infty} \; m_{XX}(\tau) = E[X(t+\tau)X(t)]$ \\

\end{tabular}

\subsection{Spektrale Leistungsdichte}
\begin{itemize}
\item $S_{XX}(\omega) \hat {=} \int_{-\infty}^\infty m_{XX}(\tau) e^{-j\omega \tau}$
\item $ m_{XX}(\tau) = \frac{1}{2\pi} \int _{-\infty}^\infty S_{XX}(\omega)e^{j\omega \tau} d\omega$
\item $S_{XX}(\omega)$ ist reell
\item $S_{XX}(\omega) = S_{XX}(-\omega)$, weil X(t) reell ist
\item $S_{XX}(\omega) \geq 0$
\item $S_{XY}(\omega) \hat {=} \int_{-\infty}^\infty m_{XY}(\tau) e^{-j\omega \tau}$
\end{itemize}




\subsection{LTI}
\begin{tabular}{ c |c }
Zeitbereich & Frequenzbereich \\\hline
Impulsantwort $h(t)$ & Frequenzantwort $H(\omega)$\\
Kreuzkorrelationen & Spektrale Kreuz-Leistungsdichten \\
$m_{XY}(\tau) = m_{XX}(\tau)*h(-\tau)$ & $S_{XY}(\omega) =S_{XX}(\omega)H^*(\omega)$ \\
$m_{YX}(\tau) = h(\tau)*m_{XX}(\tau)$ & $S_{YX}(\omega)=H(\omega)S_{XX}(\omega)$ \\
Korrelation & Spektrale Leistungsdichte \\
$m_{YY}(\tau) = h(\tau)*m_{XX}(\tau)*h(-\tau)$ & $S_{YY}(\omega) = |H(\omega)|^2S_{XX}(\omega)$\\ \hline
Ausgangsmittel & Ausgangsleistung \\
$\mu_Y= \mu_X \int_{-\infty}^\infty h(\tau) d\tau = \mu_XH(0)$ & $E\{Y^2(t)\} = m_{YY}(0) = \frac{1}{2\pi}\int_{-\infty}^\infty |H(\omega)|^2S_{XX}(\omega) d\omega$ \\
\end{tabular}










\end{document}